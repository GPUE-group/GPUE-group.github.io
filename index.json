[
{
	"uri": "https://gpue-group.github.io/data_analysis/output/",
	"title": "GPUE Output Format",
	"tags": [],
	"description": "",
	"content": " GPUE outputs data in a simple ASCII format where all arrays have been re-indexed and output element-by-element in a 1 dimensional array. For 1 dimensional simulations, these values can be immediately plotted with separate plotters like gnuplot. For 2 and 3 dimensional simulations, a certain amount of data analysis is required; however, this should be straightforward depending on the application.\nGPUE will automatically output the following files unless the -f flag is used:\n V_0: The potential of the first timestep K_0: The momentum-space components used in the split-operator method Ax_0, Ay_0, Az_0: The gauge field in the x, y, and z directions Bx_0, By_0, Bz_0: The magnetic field in the x, y, and z directions  Note that if the -J flag is used, the magnetic fields will be output in polar coordinates.\nIf the -W flag is used, the wavefunction will be output every printstep, which is set by the -p flag. For example:\n./gpue -e 1000 -W -p 100  will run GPUE in real-time for 1000 steps and output the wavefunction every timestep. In 2 dimensions, this will also output vortex tracking values and in 3 dimensions, this will output an additional Edges_n file, which is the sobel filter of the wavefunction density. The Edges_n file can be used for vortex highlighting in 3 dimensions.\nThe -d flag sets the data directory relative to the gpue executable. If the data directory does not exist, that directory will be created.\nReshaping arrays for usage in various scripting languages If GPUE is run in 2 or 3 dimensions and it is necessary to analyze the data, it may be worthwhile to reshape the data. For example, in python, this might look like:\n data = data_dir + \u0026quot;/\u0026quot; + val lines = np.loadtxt(data) A = np.reshape(lines, (xDim,yDim,zDim)); return A  This will create a 3 dimensional array that can be read as A[i,j,k]. A simlar procedure must be done in 2 dimensions, simply dropping the k index and zDim from the array indexing and reshaping function, respectively.\nParams.dat file GPUE holds many variables in an unordered map for usage within the code, itself. This allows us to keep a name associated with each variable for dynamic equation parsing and also allows us to output each variable into a single file for data analysis later. All of these variables can be found in the Params.dat file in the data directory. If the user needs a single variable for data analysis (such as xDim or dx), they can find it there. An example can be found here:\n[Params] dpz=125664 sepMinEpsilon=0 y0_shift=0 a0z=7.62574e-06 x0_shift=0 dy=1.95313e-07 fudge=1 winding=0 mask_2d=0.00015 yMax=2.5e-05 laser_power=0 gdt=0.0001 omegaX=6.283 xMax=2.5e-05 gammaY=1 omega=1 z0_shift=0 omegaZ=6.283 dpy=125664 DX=0 box_size=2.5e-05 interaction=1 thresh_const=1 omegaY=6.283 mass=1.44316e-25 a_s=4.76e-09 angle_sweep=0 a0x=7.62574e-06 a0y=7.62574e-06 dpx=125664 gDenConst=4.6095e-51 dt=0.0001 Rxy=0.36659 zMax=2.5e-05 pyMax=1.6085e+07 pxMax=1.6085e+07 pzMax=1.6085e+07 dz=1.95313e-07 dx=1.95313e-07 plan_dim3=5 plan_3d=6 plan_other2d=2 yDim=256 kill_idx=-1 gsteps=1 esteps=1 zDim=256 plan_dim2=4 plan_2d=1 atoms=1 xDim=256 dimnum=3 printSteps=100 kick_it=0 ramp_type=1 gSize=16777216 device=2 charge=0 result=0 plan_1d=3  "
},
{
	"uri": "https://gpue-group.github.io/development/gauge/",
	"title": "Gauge Fields",
	"tags": [],
	"description": "",
	"content": " Title 1  Stuff goes here\n"
},
{
	"uri": "https://gpue-group.github.io/intro/",
	"title": "Introduction to GPUE",
	"tags": [],
	"description": "",
	"content": "   \n  GPUE is a Gross\u0026mdash;Pitaevskii equation solver that is accelerated on GPU hardware with CUDA. Though the project began as a general method to study vortex dynamics in 2 dimensional Bose\u0026mdash;Einstein Condensates (BECs), it has since grown into a general-purpose BEC simulator using the Split-Operator method for 1, 2, and 3 dimensions and allows for dynamic gauge fields and potentials. The purpose of this documentation is to provide a general introduction to running GPUE for various purposes and provide a detailed guide for those wishing to develop GPUE in the future.\nGPUE supports linux and MacOS environments and has been developed primarily for HPC computing Tesla-series Nvidia compute devices. Though other platforms may run GPUE, they are not officially supported.\n"
},
{
	"uri": "https://gpue-group.github.io/functionality/running_gpue/",
	"title": "Running GPUE",
	"tags": [],
	"description": "",
	"content": "  After building GPUE, it is important to run the unit tests with the ./gpue -u command and make sure all tests pass. If they do not, there could be a failure in the building process or a problem with running GPUE on the provided hardware.\nAfter the unit tests have passed, you are free to use GPUE to your heart\u0026rsquo;s content. The easiest way to run GPUE is by using the provided help menu with the ./gpue -h command and creating a command from that; however, a parameter file can also be used for this purpose if it is easier.\nThis section provides an introduction to the unit tests and a description of the help menu along with some example simulations to run.\nUnit tests GPUE provides a suite of unit tests for the following operations:\n Math Operator Test: A suite of tests for all in-built mathematical operators Double2 Functions Test: A suite of tests for all in-built operators on CufftDoubleComplex or double2 types Grid Test: This tests to ensures the gpu is functional and simply sends data back and forth to the GPU using a specified number of threads Parallel Summation Test: this tests the parallel summation routine necessary for imaginary-time evolution FFT Test: This is a simple test of the CUFFT library and makes sure that all plans used for FFT operations work Dynamic Test: This is a test of all Abstract Syntax Tree (AST) functions necessary for dynamic fields and potentials. Bessel Test: this is a test of the polynomial approximation to the bessel functions used in the AST data structure Make Complex Test: A test of the make_complex function that turns potentials into operators for evolution cMultPhi_test: Test of the cMultPhi kernel for evolution with an imprinted Phi parameter 10.*Evolution Test: Test of imaginary and real0time evolution in 1, 2, and 3 dimensions. This first runs the simulation in imaginary time to ensure the appropriate energy for a single particle in the ground state of an n-dimensional harmonic oscillator and then runs the simulation in real-time to ensure the energy does not vary from this value.  These are all done with the ./gpue -u command. Please run this command after building GPUE to test your installation.\nThe most important of these tests is the Evolution Test. This one ensures the physical accuracy of the results, so long as the appropriate energy is found in the ground state of the harmonic oscillator, the code should be working as-intended for simple evolution. If any test fails, please create an issue on GitHub\nHelp menu To run GPUE, simply use the ./gpue command with appropriate flags for the desired simulation. To determine which flags should be used, please use the ./gpue -h command, which will output the following:\n GPUE Graphics-Processing Unit Gross--Piteavskii Equation solver Options: -A rotation Set gauge field mode [beta] -a Set flag to graph [deprecated] -b 2.5e-5 Set box size (xyzMax in 3d) -C 0 Set device (Card) for GPU computing -c 2 Set coords (1, 2, 3 dimensions) -D 0.0 Set's offset for kill (-K flag) distance radially -d data Set data directory - where to store / read data (here in data/) -E Perform energy calculation every writing step -e 1 Set esteps, number of real-time evolution steps -f Unset write to file flag -G 1 Set GammaY, ratio of omega_y to omega_x -g 1 Set gsteps, number of imaginary-time evolution steps -H phrase Print help menu, searching for \u0026quot;phrase\u0026quot; -h Print help menu, exit GPUE -I \u0026quot;param.cfg\u0026quot; Set Input parameter file -i 1 Set interaction strength between particles -j 1 Set threshold multiplier for easier vortex detection -J Set cylindrical coordinate output for B-field (no letters) -K 0 Selects vortex with specified ID to be killed/flipped/mutliply-charged -k 0 Set kick_it, kicking for Moire lattice simulations 0 = off, 1 = periodic kicking, 2 = single kick -L 0 Set l, vortex winding [2*pi*L] -l Set ang_mom flag to use angular momentum -m Set 2d_mask for vortex tracking -n 1 Set N, number of particles in simulation -O 0 Set angle_sweep, kicking potential rotation angle -P 0 Set laser_power, strength of kicking [hbar * omega_perp] -p 100 Set printSteps, frequency of printing -Q 0 Set z0_shift, z shift of kicking potential (bad flag, I know) -q 0 Set the vortex winding for imprinting/annihilation/flipping during real-time. The value is the charge of the new vortex -R 1 Set ramping flag for imaginary time evolution 1 for ramping up, 0 for ramping down -r Set read_wfc to read wavefunction from file -S 0 Set sepMinEpsilon, kicking potential lattice spacing scaling -s Set gpe, flag for using Gross-Pitaevskii Equation -T 1e-4 Set gdt, timestep for imaginary-time -t 1e-4 Set dt, timestep for real-time -U 0 Set x0_shift, x shift of kicking potential, shift of the imprinting vortex position defined by K and q -u Performs all unit tests -V 0 Set y0_shift, y shift of kicking potential, shift of the imprinting vortex position defined by K and q -v Set potential [beta] -W Set write_it, flag to write to file -w 0 Set Omega_z, rotation rate relative to omega_perp This acts as a value to multiply the gauge fields by -X 6.283 Set omega_x -x 256 Set xDim, dimension in X (power of 2, please!) -Y 6.283 Set omega_y -y 256 Set yDim, dimension in Y (power of 2, please!) -Z 6.283 Set omega_z, confinement in z-dimension -z 256 Set zDim, dimension in Z (power of 2, please!) Notes: - Parameters specified above represent the default values for the classic linear Schrodinger equation and may need to be modified to match your specific problem. - We use real units with an Rb87 condensate - You may generate a simple vortex lattice in 2d with the following command: ./gpue -x 512 -y 512 -g 50000 -e 1 -p 5000 -W -w 0.5 -n 1e5 -s -l -Z 10 - Thanks for using GPUE!  If you would like to search this for a particular flag, please use the ./gpue -H search_variable command. For example, if I wanted to change omega_x I might search for it like so:\n./gpue -H omega  which will provide the following:\n-G 1 Set GammaY, ratio of omega_y to omega_x -P 0 Set laser_power, strength of kicking [hbar * omega_perp] -w 0 Set Omega_z, rotation rate relative to omega_perp -X 6.283 Set omega_x -Y 6.283 Set omega_y -Z 6.283 Set omega_z, confinement in z-dimension  This will make it easier to find the flags you might want for the simulation.\nSpecial considerations There are several flags that need special consideration here:\n -r: This flag will read in a wavefunction from file and will naturally look for it in the data/wfc_load and data_wfci_load files. If these files do not exist, GPUE will ask for appropriate filenames. If the file lengths do not match the dimensions of the simulation, it will ask for new filenames again. -A: This flag will read in a gauge field, which can be rotation, constant, test, or file. If the gauge field is being read in from a file, it will look for the field in the data/Axgauge, data/Aygauge, and data/Azgauge files. If they do not exist or are of the wrong dimensions, GPUE will prompt for a new file. -v: This flag is naturally set to be a harmonic oscillator, but can use the torus option in 3D. Otherwise, the flag\u0026rsquo;s default will be harmonic -I: This flag will read in a configuration file for dynamic fields, which will be described later in the documentation -d: This flag will specify a data directory. All configuration files are expected to be in this directory. If the directory does not exist at the start of the simulation, GPUE will create the directory.  Example simulations Examples to come!\n"
},
{
	"uri": "https://gpue-group.github.io/development/ast/",
	"title": "Abstract Syntax Trees",
	"tags": [],
	"description": "",
	"content": " Title 1  Stuff goes here\n"
},
{
	"uri": "https://gpue-group.github.io/build/",
	"title": "Building GPUE",
	"tags": [],
	"description": "",
	"content": "   \n  GPUE is designed with both a traditional Makefile and CMake, both of which may be used for different purposes, depending on the hardware available.\nDependencies and Hardware We attempted to keep the dependency list small for GPUE, so we only require the following dependencies:\n CUDA (version $\u0026gt;=$ 7.5.18) CUFFT (bundled with CUDA) GCC or clang CMake (version $\u0026gt;=$ 3.8; optional)  GPUE performs computation almost entirely on GPU hardware, and the sm_2.0 (Fermi) to sm_7.0 (Volta) architectures are all supported. A list of all cards within these ranges can be found on wikipedia.\nMakefile Here a slightly modified GPUE Makefile:\nCUDA_HOME = /path/to/cuda/ GPU_ARCH = sm_XX OS:= $(shell uname) ifeq ($(OS),Darwin) CUDA_LIB = $(CUDA_HOME)/lib CUDA_HEADER = $(CUDA_HOME)/include CC = $(CUDA_HOME)/bin/nvcc -ccbin /usr/bin/clang --ptxas-options=-v CFLAGS = -g -std=c++11 -Wno-deprecated-gpu-targets else CUDA_LIB = $(CUDA_HOME)/lib64 CUDA_HEADER = $(CUDA_HOME)/include CC = $(CUDA_HOME)/bin/nvcc --ptxas-options=-v --compiler-options -Wall CHOSTFLAGS = #-fopenmp CFLAGS = -g -O0 -std=c++11 -Xcompiler '-std=c++11' -Xcompiler '-fopenmp' endif CUDA_FLAGS = -lcufft CLINKER = $(CC) RM = /bin/rm INCFLAGS = -I$(CUDA_HEADER) LDFLAGS = -L$(CUDA_LIB) EXECS = gpue # BINARY NAME HERE DEPS = ./include/constants.h ./include/ds.h ./include/edge.h ./include/evolution.h ./include/fileIO.h ./include/init.h ./include/kernels.h ./include/lattice.h ./include/manip.h ./include/minions.h ./include/node.h ./include/operators.h ./include/parser.h ./include/split_op.h ./include/tracker.h ./include/unit_test.h ./include/vort.h ./include/vortex_3d.h ./include/dynamic.h OBJ = fileIO.o kernels.o split_op.o tracker.o minions.o ds.o edge.o node.o lattice.o manip.o vort.o parser.o evolution.o init.o unit_test.o operators.o vortex_3d.o dynamic.o %.o: ./src/%.cc $(DEPS) $(CC) -c -o $@ $(INCFLAGS) $(CFLAGS) $(LDFLAGS) -Xcompiler \u0026quot;-fopenmp\u0026quot; -arch=$(GPU_ARCH) $\u0026lt; %.o: ./src/%.cu $(DEPS) $(CC) -c -o $@ $(INCFLAGS) $(CFLAGS) $(LDFLAGS) $(CUDA_FLAGS) -Xcompiler \u0026quot;-fopenmp\u0026quot; -arch=$(GPU_ARCH) $\u0026lt; -dc gpue: $(OBJ) $(CC) -o $@ $(INCFLAGS) $(CFLAGS) $(LDFLAGS) $(CUDA_FLAGS) -Xcompiler \u0026quot;-fopenmp\u0026quot; -arch=$(GPU_ARCH) $^ clean: @-$(RM) -f r_0 Phi_0 E* px_* py_0* xPy* xpy* ypx* x_* y_* yPx* p0* p1* p2* EKp* EVr* gpot wfc* Tpot 0* V_* K_* Vi_* Ki_* 0i* k s_* si_* *.o *~ PI* $(EXECS) $(OTHER_EXECS) *.dat *.eps *.ii *.i *cudafe* *fatbin* *hash* *module* *ptx test* vort* v_opt*;  To use the Makefile, the first 2 lines must be modified to reflect your desired cuda/ path and the architecture of your GPU device. After this, simply make to build the code. If rebuilding is necessary, run make clean then make.\nIf you are developing GPUE, this file will need to be modified as new code is developed.\nCMake CMake is the preferred building system for GPUE; however, if a CMake version $\u0026gt;=$ 3.8 must be used. In this case, run\ncmake .  in the primary GPUE directory and then run make as in the Makefile example. make clean will clean the directory for rebuilding.\nTesting GPUE Once GPUE is built, please run unit tests with ./gpue -u and make sure everything passes. If there is a failure in the build, please create an issue on GitHub.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  "
},
{
	"uri": "https://gpue-group.github.io/data_analysis/python/",
	"title": "Python Scripts",
	"tags": [],
	"description": "",
	"content": " GPUE provides a number of python scripts in the py/ directory. This page explains what each script is and its intended use.\nAs a note: GPUE is primarily a simulation program. Data analysis of this nature must be done on a case-by-case basis, and as such, these scripts only intend to provide basic functionality for plotting the output wavefunction and additional variables. If more advanced functions are required, please create these on your own or discuss them with us on GitHub\nplot.py This file is a stand-alone plotting script for 2 dimensional data and can be run like so:\npython plot.py i wfc g 256 256 r 0 1000 100 d data  This will plot the wavefunction from imaginary-time evolution using a grid of 256x256 from values 0 to 1000 in steps of 100. Obviously, the code requires these data files to exist in the data directory before plotting. The script will default to plotting only a single 512x512 data file from the data directory if no gridsize, range, or directory is provided.\nAll 2 dimensional variables can be plotted in this way. If real-time dynamics are desired, simply use the wfc_ev flag instead of wfc.\ngen_data.py This file is a list of functions necessary to analyze 3 dimensional data, and an example of how it might be used can be found in the vis_scripts.py file.\nHere are the primary functions and their intended usage:\nwfc_density(xDim, yDim, zDim, data_dir, pltval, i)  This function outputs an item that corresponds to the wavefunction density. This item can be turned into a .bvox or .vtk file with the to_bvox() and to_vtk() functions described later. The pltval argument can read either wfc for imaginary-time evolution or wfc_ev for real-time evolution.\nwfc_phase(xDim, yDim, zDim, data_dir, pltval, i)  This function outputs an item that corresponds to the wavefunction phase. This item can be turned into a .bvox or .vtk file with the to_bvox() and to_vtk() functions described later. The pltval argument can read either wfc for imaginary-time evolution or wfc_ev for real-time evolution.\nproj_phase_2d(xDim, yDim, zDim, data_dir, pltval, i)  This slices the wavefunction along the z-axis and outputs the phase as a wfc_ph_i file, where i corresponds to the value read into this function. The pltval argument can read either wfc for imaginary-time evolution or wfc_ev for real-time evolution. This file can be plotted with plot.py, for example.\nproj_2d(xDim, yDim, zDim, data_dir, pltval, i)  This slices the wavefunction along the z-axis and outputs the density as a wfc_i file, where i corresponds to the value read into this function. The pltval argument can read either wfc for imaginary-time evolution or wfc_ev for real-time evolution. This file can be plotted with plot.py, for example.\nvar(xDim, yDim, zDim, data_dir, pltval)  This function outputs an item that corresponds to the variable to be plotted. This item can be turned into a .bvox or .vtk file with the to_bvox() and to_vtk() functions described later. The pltval argument can be any existing file in the data_directory that is formatted for 3 dimensional output.\nproj_var2d(xdim, yDim, zDim, data_dir, pltval, file_string)  This slices the 3 dimensional variable along the z-axis and outputs the phase as a file with the name file_string This file can be plotted with plot.py, for example.\nproj_var1d(xdim, yDim, zDim, data_dir, pltval, file_string)  This slices the 3 dimensional variable along the z and y axes and outputs the phase as a file with the name file_string This file can be plotted with any standard plotter, like gnuplot.\nto_bvox(item, xDim, yDim, zDim, nframes, filename)  This takes an item output from the wfc_phase(), wfc_density(), or var() functions and turns it into a .bvox file with name filename to be read by blender. The nframes function corresponds to the number of frames blender will plot. For almost all cases, this should be 1. The .bvox file can be used further with the visualize_3d.py file.\nto_vtk(item, xDim, yDim, zDim, data_dir, filename)  This takes an item output from the wfc_phase(), wfc_density(), or var() functions and turns it into a .vtk file with name filename to be read by blender. This can be directly read into 3 dimensional plotters like Paraview and is the preferred method to visualize data in 3 dimensions.\nvisualize_3d.py This file works with the blender bpy library to create a 3 dimensional image from a .bvox file and the voxelfile and outfile must be modified depending on the file input and output.\nIt is run with the following command:\nblender -b -P visualize_3d.py  To open up the blender GUI with the density plotted, run:\nblender -P visualize_3d.py  en.py This is an example script of how to find the energy of a 2 dimensional simulation from GPUE. As a note, this functionality is already completely implemented in GPUE by outputting the energy every timestep with the -E flag; however, this script provides an example of how this can be done with python.\n"
},
{
	"uri": "https://gpue-group.github.io/functionality/",
	"title": "GPUE functionality",
	"tags": [],
	"description": "",
	"content": "GPUE is a general-purpose BEC simulator and can be used for a number of different superfluid simulations. The basic usage of GPUE will be described here, including:\n Basic usage of GPUE for 1, 2, and 3 dimensional simulations Gauge fields in 2 and 3 dimensions Vortex tracking in 2 dimensions Dynamic fields and equation parsing  "
},
{
	"uri": "https://gpue-group.github.io/functionality/gauge/",
	"title": "Gauge Fields",
	"tags": [],
	"description": "",
	"content": "One notable feature of GPUE is its ability to use gauge fields to create rotational effects in BEC simulations. Gauge fields are somewhat tricky to understand and this guide is not meant to provide a full physical understanding of what the fields are or how they work. If you with to learn more about the physical interpretation of these fields, this review by Jean Dalibard is relatively in-depth.\nThis section will highlight how gauge fields are implemented in GPUE and how to use them on your own as a user of the codebase. If you wish to provide a new default field in GPUE, itself, please go to the developer guide for this later in the documentation and provide an appropriate PR to GPUE on GitHub\n Introduction to gauge fields For the purposes of the GPUE simulations, we will be solving the non-linear Schr\u0026ouml;dinger equation:\n$$ \\frac{\\partial \\Psi(\\mathbf{r},t)}{\\partial t} = \\left( \\frac{(p-m\\mathbf{A})^2}{2m} + V_0 + g|\\Psi(\\mathbf{r},t)|^2 \\right)\\Psi(\\mathbf{r},t), $$\nwhere $\\Psi(\\mathbf{r},t)$ is a complex-valued wavefunction, $r$ is a real-space grid, $p$ is a momentum-space grid, $m$ is the mass of the atomic system, $V_0$ is a real-valued trapping potential, $g$ is a coupling constant, and $A$ is a real-valued gauge field. As mentioned, the gauge field is hard to physically interpret; however, it can be more easily understood through the artificial magnetic field, $B = \\nabla \\times A$. Here, it appears that rotation occurs around the magnetic field lines, so vortices in a BEC will follow the direction of the magnetic field at any moment in time.\nGauge fields are implemented in GPUE by splitting the operator up in position and momentum space. Because $p$ is completely in momentum space, while $A$ is in real space, the $\\frac{(p-mA)^2}{2m}$ term will have three different types of values after expanding:\n$$ \\frac{(p-mA)^2}{2m} = \\frac{p^2 + (mA)^2 + 2mpA}{2m} $$\nHere, $\\frac{p^2}{2m}$ will be in momentum-space, $\\frac{mA^2}{2}$ will be completely in position-space, and $pA$ will be in momentum-space for one dimension and position space for all other dimensions. For example, one possible gauge field can be created with the angular momentum operator as $xp_y - yp_x$.\nBecause of this, we need to perform a 1 dimensional FFT on our wavefunction in order to apply our gauge fields to the system. This means that when we run the simulation with gauge fields, we need to perform an additional FFT every timestep for each dimension of our system. In addition, we need to read in a field for each dimension. For example, with $A = xp_y - yp_x$, we will use the following field:\n$$ \\begin{align} A_x \u0026amp;= -y \\newline A_y \u0026amp;= x \\newline A_z \u0026amp;= 0 \\end{align} $$\nTo apply this field, we will need to FFT our wavefunction in first the x-direction, then multiply by $A_x$, iFFT back and do the same for $A_y$ and $A_z$. This is a costly process, which is why you should only use gauge fields if you need them.\nAs mentioned, this documentation does not intend to provide a full in-depth description of gauge fields and artificial magnetic fields. It instead attepts to provide the basics for users that may need more information about what gauge fields are and how they influence the simulation. Basically, if you want to do vortex simulations, you will need gauge fields.\nUsing gauge fields in GPUE GPUE comes packaged with a few in-built gauge fields:\n Rotation: This is used with the ./gpue -A rotation flag. Here, we apply the rotational operation to the wavefunction as $\\Omega(xp_y - yp_x)$, where $\\Omega$ is some rotation frequency. If a rotation above some critical rotation frequency is used, vortices will appear in the BEC, eventually creating a triangular vortex lattice. Constant: This is used with the ./gpue -A constant flag. Here, we simply read in $A_x = A_y = A_z = 0$. This is useful for debugging in certain cases Test: This is used with the ./gpue -A test flag. Here, we apply a constant field in $A_y$ and $A_z$ and provide a sinusoudal field in $A_x$. This is another testing field that does not use standard rotation. File: This is used with the ./gpue -A file flag. Here, we read the fields in from file. This will naturally look for the fields in data/Axgauge, data/Aygauge, and data/Azgauge. If these files are not found, it will ask for an appropriate filename. If the file length does not match the dimensions of the system, it will also ask for a new file.  To run the simulation with gauge fields, the -l flag is necessary and will turn on the part of the simulation with the additional FFT\u0026rsquo;s for applying the necessary gauge field. No gauge field will be applied without the -l flag!\nGauge fields can also be used with the dynamic fields to be covered in the next section!\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  "
},
{
	"uri": "https://gpue-group.github.io/data_analysis/matlab/",
	"title": "Matlab Scripts",
	"tags": [],
	"description": "",
	"content": " Title 1  Stuff goes here\n"
},
{
	"uri": "https://gpue-group.github.io/functionality/vortex_2d/",
	"title": "2D Vortex Tracking",
	"tags": [],
	"description": "",
	"content": " One of the major components of GPUE is the ability to track and manipulate vortices in Bose-Einstein condensates. While we can work with creating vortices in 3D, the majority of the vortex creation and manipulation framework exists solely in 2D. This is due to the 2D vortex code being developed for the project on vortex dynamics, published in PRA here (arXiV version here).\nTo allow us to track and manipulate vortices in a (2D) condensates, we require some numerical techniques and tools:\n Vortex detection. Vortex position refinement Vortex unique identification and tracking. Arbitrary phase control of the condensate. Lattice model creation.  1. Vortex detection To find a vortex in the condensate, we can rely on several methods, such as tracking the condensate density minima. However, given the nature of the wavefunction (a complex valued scalar field), and a quantum vortex (topological defect of the wavefunction), we can examine the phase of the condensate, $\\phi$, such that $\\Psi=|\\Psi|\\exp\\left(i \\phi\\right)$. Every vortex in the condensate will have integer winding in the complex plane, wherein the phase has a singular point around which it winds through $2\\pi$. By examining the condensate for these phase windings, we can identify the presence of a vortex to a location on our numerical grid.\nThe above image show the phase of a condensate containing four vortices. The zoomed region shows the numerical values of the sampled grid near this vortex core. By following a closed path around the dotted green line the the vortex core can be determined when the value is $\\pm 2\\pi$, positive for vortices, negative for anti-vortices (ie, vortices rotating in the opposite direction). In this instance, $$ \\displaystyle\\sum\\limits_{i=1}^{4} \\phi_i = 3.14 + 3.14 -2.76 -2.76 = 2\\pi. $$ With this formalism, we can identify the vortex core to the region of a $2\\times 2$ grid plaquette. However, we may also determine a better sub-grid resolution position of the core, by realising that a vortex core corresponds with a zero-crossing in the complex plane for both the real and imaginary components. Following O\u0026rsquo;Riordan, 2017\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  "
},
{
	"uri": "https://gpue-group.github.io/data_analysis/",
	"title": "Data analysis",
	"tags": [],
	"description": "",
	"content": "GPUE outputs data in a basic ASCII format so it is easy for users to read the data into an auxiliary program and analyze it as necessary; however, GPUE also provides a series of scripts for 2 and 3D analysis with the following functionality:\n Plotting in 2 dimensions Generation of 2 dimensional slices of 3 dimensional data Generation of .vbox or .vtk files in 3 dimensional for plotting in blender and paraview, respectively Generation of images that can later be concatenated into a video with standard tools (ffmpeg, ImageMagick, etc.)  GPUE is primarily a simulation program and thus provides only limited tools necessary to visualize the data. If the user requires more advanced data analysis, these must be further developed by the user for their specific research purpose.\n"
},
{
	"uri": "https://gpue-group.github.io/api/",
	"title": "API",
	"tags": [],
	"description": "",
	"content": " Title 1 DOXYGEN generated documentation\n"
},
{
	"uri": "https://gpue-group.github.io/functionality/dynamic_fields/",
	"title": "Dynamic Fields",
	"tags": [],
	"description": "",
	"content": "GPUE allows for users to input their own custom, dynamic fields that can vary with time, space, and auxiliary parameters. These fields are parsed during run-time and allow users to run GPUE with multiple different distributions without recompiling. This was done for the following reasons:\n GPUE hardware is inherently limited in memory. After some development, we found that there simply was not enough room on older GPU hardware for the wavefunction, gauge fields, and auxiliary operators for evolution. If we wanted to change these fields during evolution, that is very difficult to do on the GPU because transferring between the CPU host and GPU device is a slow process \u0026ndash; even with recent progress in NVLink technology! We cannot assume that all users are also developers. Adding new operators to GPUE was once a very difficult process that would take a few hours on the part of the developer. This obviously meant that users could only use GPUE for a few small-scale simulations and could not use it for general use. By allowing users to input their own fields during run-time, GPUE can be used for a much larger number of cases.   Using dynamic fields All dynamic fields must be specified in some configuration file to be read by the ./gpue -I file.cfg flag.\nAn example is provided in the src/example.cfg file:\nrad = 10 omegaR = 7071 V = rad*omegaR*x*y*z  Here, we specify whatever variables we like. This also allows us to redefine parameters for the simulation. The terms x, y, z, t, Ax, Ay, Az, V, and K are all reserved and recognized by the parsing scheme. All standard mathematical operations are also supported.\nAs a note: using the dynamic parser in this way saves on memory usage; however, it also slows down the simulation slightly. Dynamic fields should only be used if your simulation requires time-dependent potentials or gauge fields or if you need to save on memory.\n"
},
{
	"uri": "https://gpue-group.github.io/development/",
	"title": "Development",
	"tags": [],
	"description": "",
	"content": " Title 1  Gauge Fields Title 1 Stuff goes here\n  Abstract Syntax Trees Title 1 Stuff goes here\n   \n  Stuff goes here\n"
},
{
	"uri": "https://gpue-group.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " GPUE-group.github.io Documentation website for GPUE\n"
},
{
	"uri": "https://gpue-group.github.io/api/gpue_build/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/api/html/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " GPUE: GPU Gross-Pitaevskii Equation: Main Page      /* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3\u0026amp;dn=gpl-2.0.txt GPL-v2 */ $(document).ready(initResizable); /* @license-end */   MathJax.Hub.Config({ extensions: [\"tex2jax.js\"], jax: [\"input/TeX\",\"output/HTML-CSS\"], });      GPUE: GPU Gross-Pitaevskii Equation \u0026#160;0  GPU Gross-Pitaevskii equation solver      /* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3\u0026amp;dn=gpl-2.0.txt GPL-v2 */ var searchBox = new SearchBox(\"searchBox\", \"search\",false,'Search'); /* @license-end */    /* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3\u0026amp;dn=gpl-2.0.txt GPL-v2 */ $(function() { initMenu('',true,false,'search.php','Search'); $(document).ready(function() { init_search(); }); }); /* @license-end */       /* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3\u0026amp;dn=gpl-2.0.txt GPL-v2 */ $(document).ready(function(){initNavTree('index.html','');}); /* @license-end */     GPUE: GPU Gross-Pitaevskii Equation Documentation    Generated on Thu Aug 23 2018 16:30:33 for GPUE: GPU Gross-Pitaevskii Equation by  1.8.14      "
},
{
	"uri": "https://gpue-group.github.io/build/gpue_build/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/data_analysis/gpue_build/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/development/gpue_build/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/functionality/gpue_functionality/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/intro/gpue_intro/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/mathjax/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  "
},
{
	"uri": "https://gpue-group.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gpue-group.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]